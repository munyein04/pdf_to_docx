import io
import re
from datetime import datetime

import nltk
import pandas as pd
import streamlit as st
from spellchecker import SpellChecker

# -------------------- NLTK ì¤€ë¹„ -------------------- #
def _ensure_nltk():
    """NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ê²€ì¦ (í† í¬ë‚˜ì´ì € + í’ˆì‚¬ íƒœê±°)"""
    required_resources = [
        ("tokenizers/punkt", "punkt", True),
        ("tokenizers/punkt_tab", "punkt_tab", False),
        ("taggers/averaged_perceptron_tagger", "averaged_perceptron_tagger", True),
        ("taggers/averaged_perceptron_tagger_eng", "averaged_perceptron_tagger_eng", False),
    ]

    for path, package, required in required_resources:
        try:
            nltk.data.find(path)
        except (LookupError, OSError):
            try:
                st.write(f"Downloading NLTK '{package}'...")
                nltk.download(package, quiet=True)
            except Exception as e:
                if required:
                    raise Exception(f"Failed to download required NLTK data '{package}': {e}")
                else:
                    st.write(f"Warning: Could not download optional NLTK data '{package}': {e}")


_WORD_RE = re.compile(r"^[A-Za-z][A-Za-z'-]*$")


def analyze_spelling(text, spell_checker):
    """
    í…ìŠ¤íŠ¸ì—ì„œ ìŠ¤í ë§ ì˜¤ë¥˜ë¥¼ íƒì§€í•˜ê³ ,
    ê° ì˜¤ë¥˜ì— ëŒ€í•´ (êµì •ì–´, í’ˆì‚¬ íƒœê·¸, ì „ì²´ ì˜¤ë¥˜ ê°œìˆ˜)ë¥¼ ë°˜í™˜.
    - corrections: {ë‹¨ì–´(lower): êµì •ì–´}
    - pos_map: {ë‹¨ì–´(lower): í’ˆì‚¬ íƒœê·¸ ë¬¸ìì—´}
    """
    words = nltk.word_tokenize(text)
    tokens = [w for w in words if _WORD_RE.match(w)]
    lowers = [w.lower() for w in tokens]

    misspelled = spell_checker.unknown(lowers)

    tagged = nltk.pos_tag(tokens)

    pos_counts = {}
    for tok, tag in tagged:
        key = tok.lower()
        if key not in pos_counts:
            pos_counts[key] = {}
        pos_counts[key][tag] = pos_counts[key].get(tag, 0) + 1

    pos_map = {}
    for w in misspelled:
        tag_dict = pos_counts.get(w, {})
        if tag_dict:
            best_tag = max(tag_dict.items(), key=lambda x: x[1])[0]
            pos_map[w] = best_tag
        else:
            pos_map[w] = ""

    corrections = {w: spell_checker.correction(w) for w in misspelled}
    return corrections, pos_map, len(misspelled)


@st.cache_resource
def get_spellchecker():
    _ensure_nltk()
    return SpellChecker()


def main():
    st.set_page_config(
        page_title="YONSEI SPELLING DETECT TOOL",
        layout="wide",
    )

    st.title("YONSEI SPELLING DETECT TOOL")
    st.write(
        "ì—¬ëŸ¬ ê°œì˜ `.txt` íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´, ìŠ¤í ë§ ì˜¤ë¥˜ì™€ í’ˆì‚¬(Word Class), êµì •ì–´ë¥¼ í•œ ë²ˆì— í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )

    uploaded_files = st.file_uploader(
        "ë¶„ì„í•  .txt íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (ì—¬ëŸ¬ ê°œ ì„ íƒ ê°€ëŠ¥)", type=["txt"], accept_multiple_files=True
    )

    run = st.button("ğŸš€ Run Spelling Detection")

    if run:
        if not uploaded_files:
            st.warning("ë¨¼ì € .txt íŒŒì¼ì„ í•˜ë‚˜ ì´ìƒ ì—…ë¡œë“œí•˜ì„¸ìš”.")
            return

        spell = get_spellchecker()
        all_rows = []

        progress = st.progress(0.0)
        total = len(uploaded_files)

        for idx, uploaded in enumerate(uploaded_files, start=1):
            raw = uploaded.read()

            text = None
            for enc in ("utf-8", "cp949", "euc-kr", "latin-1"):
                try:
                    text = raw.decode(enc)
                    break
                except UnicodeDecodeError:
                    continue

            if text is None:
                st.warning(f"âš ï¸ {uploaded.name} - ì¸ì½”ë”© ì˜¤ë¥˜ë¡œ ê±´ë„ˆëœ€")
                progress.progress(idx / total)
                continue

            corrections, pos_map, miss_count = analyze_spelling(text, spell)

            for err, corr in corrections.items():
                all_rows.append(
                    {
                        "file": uploaded.name,
                        "spelling_error": err,
                        "word_class": pos_map.get(err, ""),
                        "correction": corr if corr else "",
                    }
                )

            progress.progress(idx / total)

        if not all_rows:
            st.info("ìŠ¤í ë§ ì˜¤ë¥˜ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ê±°ë‚˜, ë¶„ì„ ê°€ëŠ¥í•œ ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        df = pd.DataFrame(all_rows)
        st.subheader("Detected Spelling Errors")
        st.dataframe(df, use_container_width=True)

        csv = df.to_csv(index=False, encoding="utf-8-sig")
        st.download_button(
            label="ğŸ“Š Download CSV",
            data=csv,
            file_name=f"yonsei_spelling_errors_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv",
        )


if __name__ == "__main__":
    main()
